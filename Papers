1.HeteroFL
	• 发表于ICLR 2021。
	• 对于r=0.5的客户端：
		○ 卷积层：卷积核的个数减少一半，卷积层的尺寸不变，因此模型参数量减少一半。每个卷积核带一个偏置，因此偏置的数量也减少一半，但是几乎可以忽略不计。
		○ 全连接层：矩阵w的高宽都减少一半，参数量变成四分之一。偏置b的长度减少一半，参数量减少一半。总之，参数量大约变成四分之一。
		○ 最后一层全连接层：矩阵w的高不变，宽减少一半，参数量减少一半。偏置b的长度不变，参数量不变。总之，参数量减少一半。
	• 注意缩放：类似于Dropout，训练是模型的一部分，因此部分模型（训练）和全局模型（预测）的输出期望不一致，需要缩放。

2.FjORD
	• 发表于NeurIPS 2021。
	• Ordered Dropout：p=0.2相当于保留前20%神经元/通道，p=0.5相当于保留前50%。
	• FjORD结合了Ordered Dropout和KD：
每个客户端分配一个p_max，在训练的时候不仅训练p_max，还会训练p<p_max。
	训练p<p_max的时候采用知识蒸馏，用p_max模型去指导p模型。
	注意p_max教师网络也是未完全训练的网络，因此也需要反向传播。
	• 在每个客户端进行预测推理的时候，都用p_max模型进行推理，那些丢失的神经元/通道被训练的少，可以看做放弃最不重要的神经元来符合客户端的负载，这是一种正则化。
	• FjORD和HeteroFL相比，最大的区别在于FjORD不仅仅会训练p_max模型，还会训练p<p_max的模型，而HeteroFL只会训练p_max模型，这避免了一个现象导致的不良影响：即高性能设备上的数据，其训练导致的模型参数会分散在整个网络中，从而导致分发给低性能设备的网络无法很好地推测这部分数据。另一个重要的区别在于，FjORD在训练p<p_max模型的时候引入了KD。
	
3.SPLIT-MIX
	• 发表于ICLR 2022。
	• SPLIT：指定一个模型颗粒度 r，例如 r=0.25，那么整个模型就可以被划分为 4 个子模型。
	MIX：每个客户端都有自己的承受能力 R，例如 R=0.75，那么该客户端会被分配 3 个子模型。在训练时，客户端会并行训练这 3 个子模型。在推理时，客户端会将这 3 个子模型合并作为最终的网络。
	• SPLIT-MIX不仅解决了上述的资源动态性，还解决了鲁棒动态性：通过鲁邦模型和普通模型的参数共享，只改变使用不同的BN层（cleanBN和noiseBN）。一般训练鲁邦模型会采用对抗样本，而SPLIT-MIX则在此基础上改变了BN。在需要鲁棒性的场景下，将BN层改变为训练好的noiseBN即可。
	• 实验部分：本文引入了SHeteroFL作为baseline，让客户端不仅训练p_max模型，还训练p<p_max模型，这和FjORD的思想非常类似。另外本文实现了两种non-IID实验：
		○ 类别 non-IID 实验：每个客户端仅有一部分标签的数据。例如CIFAR10被划分给100个客户端，每个客户端只有三个类别的数据。
		○ 特征 non-IID 实验：
		Digits 是 FL 的常用基准，包括了 MNIST / SVHN 等5个“域”。
		DomainNet 包含了真实图像、素描、绘画等6个“域”。
		特征 non-IID 数据集的特性是：图片长得很不一样，但他们的标签是一样的。
		例如Digits的每个域被划分给10个客户端，那么总共有50个客户端，每个客户端只有一个域的图片。
	• 有趣的结论和推测：
		○ 从访问数据的角度来看：HeteroFL的*0.25网络只能访问最低性能设备上的数据，*1网络只能访问最高性能设备上的数据。SHeteroFL和FjORD的*0.25网络可以访问所有数据，*1网络只能访问最高性能设备上的数据，这会不会导致*1网络没有收敛，出现*1网络的性能甚至不如*0.25网络？SPLIT-MIX的每个子网都可以访问几乎所有数据，并且都是小子网，收敛很快。
		○ 从TODO角度来看：TODO
	• 论文的Figure7展示了一种高级的可视化技术。

4.FEDLITE
	• ICLR2022 被拒。
	• 采用SPLIT-NN：客户端训练一部分，将中间层激活传给服务器，让服务器完成后半部分的训练，将对激活的导数传回客户端用于更新参数，用到链式法则。数据仍然在客户端本地。
	• 虽然SPLIT-NN解决了计算和内存问题，但是传输中间层激活是一个很大的通信开销。因此引入PQ算法，进行分段聚类。本文的主要工作是改进了PQ算法，引入了Stack阶段，进一步降低了通信开销。同时引入泰勒展开，降低精度损失。
	• 论文的图3和图4展示了一种高级的可视化技术。

5.FedResCuE
	• 发表于ICML 2022。
