1.HeteroFL
	• 发表于ICLR 2021。
	• 对于r=0.5的客户端：
		○ 卷积层：卷积核的个数减少一半，卷积层的尺寸不变，因此模型参数量减少一半。每个卷积核带一个偏置，因此偏置的数量也减少一半，但是几乎可以忽略不计。
		○ 全连接层：矩阵w的高宽都减少一半，参数量变成四分之一。偏置b的长度减少一半，参数量减少一半。总之，参数量大约变成四分之一。
		○ 最后一层全连接层：矩阵w的高不变，宽减少一半，参数量减少一半。偏置b的长度不变，参数量不变。总之，参数量减少一半。
	• 注意缩放：类似于Dropout，训练是模型的一部分，因此部分模型（训练）和全局模型（预测）的输出期望不一致，需要缩放。

2.FjORD
	• 发表于NeurIPS 2021。
	• Ordered Dropout：p=0.2相当于保留前20%神经元/通道，p=0.5相当于保留前50%。
	• FjORD结合了Ordered Dropout和KD：
		○ 每个客户端分配一个p_max，在训练的时候不仅训练p_max，还会训练p<p_max。
		○ 训练p<p_max的时候采用知识蒸馏，用p_max模型去指导p模型。
		○ 注意p_max教师网络也是未完全训练的网络，因此也需要反向传播。
	• 在每个客户端进行预测推理的时候，都用p_max模型进行推理，那些丢失的神经元/通道被训练的少，可以看做放弃最不重要的神经元来符合客户端的负载，这是一种正则化。
	• FjORD和HeteroFL相比，最大的区别在于FjORD不仅仅会训练p_max模型，还会训练p<p_max的模型，而HeteroFL只会训练p_max模型，这避免了一个现象导致的不良影响：即高性能设备上的数据，其训练导致的模型参数会分散在整个网络中，从而导致分发给低性能设备的网络无法很好地推测这部分数据。另一个重要的区别在于，FjORD在训练p<p_max模型的时候引入了KD。
	
3.SPLIT-MIX
	• 发表于ICLR 2022。
	• SPLIT：指定一个模型颗粒度 r，例如 r=0.25，那么整个模型就可以被划分为 4 个子模型。
	• MIX：每个客户端都有自己的承受能力 R，例如 R=0.75，那么该客户端会被分配 3 个子模型。在训练时，客户端会并行训练这 3 个子模型。在推理时，客户端会将这 3 个子模型合并作为最终的网络。
	• SPLIT-MIX不仅解决了上述的资源动态性，还解决了鲁棒动态性：通过鲁邦模型和普通模型的参数共享，只改变使用不同的BN层（cleanBN和noiseBN）。一般训练鲁邦模型会采用对抗样本，而SPLIT-MIX则在此基础上改变了BN。在需要鲁棒性的场景下，将BN层改变为训练好的noiseBN即可。
	• 实验部分：本文引入了SHeteroFL作为baseline，让客户端不仅训练p_max模型，还训练p<p_max模型，这和FjORD的思想非常类似。另外本文实现了两种non-IID实验：
		○ 类别 non-IID 实验：每个客户端仅有一部分标签的数据。例如CIFAR10被划分给100个客户端，每个客户端只有三个类别的数据。
		○ 特征 non-IID 实验：
		Digits 是 FL 的常用基准，包括了 MNIST / SVHN 等5个“域”。
		DomainNet 包含了真实图像、素描、绘画等6个“域”。
		特征 non-IID 数据集的特性是：图片长得很不一样，但他们的标签是一样的。
		例如Digits的每个域被划分给10个客户端，那么总共有50个客户端，每个客户端只有一个域的图片。
	• 有趣的结论和推测：
		○ 从访问数据的角度来看：HeteroFL的*0.25网络只能访问最低性能设备上的数据，*1网络只能访问最高性能设备上的数据。SHeteroFL和FjORD的*0.25网络可以访问所有数据，*1网络只能访问最高性能设备上的数据，这会不会导致*1网络没有收敛，出现*1网络的性能甚至不如*0.25网络？SPLIT-MIX的每个子网都可以访问几乎所有数据，并且都是小子网，收敛很快。
		○ 从TODO角度来看：TODO
	• 论文的图7展示了一种高级的可视化技术。

4.FEDLITE
	• ICLR2022 被拒。
	• 采用SPLIT-NN：客户端训练一部分，将中间层激活传给服务器，让服务器完成后半部分的训练，将对激活的导数传回客户端用于更新参数，用到链式法则。数据仍然在客户端本地。
	• 虽然SPLIT-NN解决了计算和内存问题，但是传输中间层激活是一个很大的通信开销。因此引入PQ算法，进行分段聚类。本文的主要工作是改进了PQ算法，引入了Stack阶段，进一步降低了通信开销。同时引入泰勒展开，降低精度损失。
	• 论文的图3和图4展示了一种高级的可视化技术。

5.FedResCuE
	• 发表于ICML 2022。
	• 与FjORD极其类似，在它的基础上引入了渐进式训练：
		○ 按照顺序在每个客户端上训练p<p_max的模型，例如p_1<p_2<…<p_max
		○ 训练p_2的时候，固定p_1的参数
	• 还引入了通信成功部分的场景：
		○ 如果客户端下载模型有一部分失败了，失败的部分用上轮的模型替代
		○ 如果客户端上传模型有一部分失败了，服务器只用上传成功的部分做聚合
	• 论文同样发现了，*0.75网的效果比*1网络要好。

6.FeDepth
	• 发表于MLSYS 2023（FLSYS workshop）。
	• 论文测试了之前的方法：
		○ 之前的这些方法存在小尺寸模型表达能力不足的问题，因此本文考虑用全尺寸模型。
		○ 论文中提到HeteroFL的部分模型参数训练不足，因为只有部分客户端可以训练全尺寸模型，这与之前提到*1模型的效果更差是一致的。另外小模型的参数规模小，容易被忽略。论文还提到SPLITMIX解决了这些问题，它增加了网络可以访问的数据。
		○ 论文提到，在DL中占据内存绝大部分的是中间层激活，但HeteroFL和SPLITMIX只考虑了模型的参数。
		○ 论文提到，在HeteroFL中，小模型（低性能客户端）对整体模型的贡献是负的，甚至出现only *1 clients的效果是最好的，而with all clients的效果是最差的。在SPLITMIX中，模型颗粒度r越小，整体模型的效果越差，即太过弱的学习器会导致一个泛化性能很差的集成模型。
		○ 论文实验表明，SPLITMIX对极度non-IID的表现非常差，因为non-IID导致了客户端之间的权重差异，而小模型无法捕获到代表性的特征。
	• 论文中给出了估计内存消耗的论文：Gao, Y., Liu, Y., Zhang, H., Li, Z., Zhu, Y., Lin, H., and Yang, M. Estimating gpu memory consumption of deep learning models. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pp. 1342–1352, 2020.
	• 论文中用到了两种NONIID设置，详情请看 https://github.com/TsingZ0/PFL-Non-IID 以及联邦学习：按Dirichlet分布划分Non-IID数据集 - 知乎 (zhihu.com)
		○ 病态NONIID：即HETEROFL和SPLITMIX论文中的试验设置，每个客户端只有N类标签的数据。
		○ 狄利克雷NONIID：实际中的独立同分布。
	• 实验结果表明，VIT对NONIID的表现更好。

7.DepthFL
	• 发表于ICLR 2023。

8.InclusiveFL
	• 发表于KDD 2022。
	• 模型结构：
		○ 将客户端分成低A、中B、高C三组。三组采用不同深度的模型，A组的客户端训练好后将梯度g传到服务器，并平均为ga，并进行更新整个模型Wa，B/C两组同理。随后A/B/C三组对模型低层的参数执行FedAvg，并在三组内共享这La层的参数。B/C两组对模型中间层的参数执行FedAvg，并在三组内共享这Lb-La层的参数。
		○ 模型还增加了动量蒸馏：例如，在获得ga后，不直接利用ga去更新Wa，而是利用上一轮通信中，B组的梯度知识去“指导”ga。这个技巧略微提高了模型的性能。
	• 论文测试的场景是cross-silo，而不是物联网场景。
	• 论文提到，HeteroFL在复杂模型场景下（transformer）表现甚至不如统一采用最小模型和排除低端设备只训练最大模型这两种方法。这可能说明Hetero只适用于CNN等简单模型。
	• 这个博客也讲了这篇论文，可以看他的试验部分讲解：人大、微软等提出InclusiveFL：异构设备上的包容性联邦学习-CSDN博客
	
9.NeFL
	• 还未发表。
	
